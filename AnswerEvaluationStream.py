from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import torch

def get_sentence_embeddings(sentences):

    #Load pre-trained Bert Model and tokenizer
    #bert have its own embedding layer so it can get encodings for the numerical tokens generated by the tokenizer
    #pre-trained tokenizer is already have some rule for tokenizing and have lasrge corpus of words already mapped 
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    #Tokenize and encode the sentence
    inputs = tokenizer( sentences, return_tensors = 'pt', padding = True, truncation=True, max_length = 512 )


    #Get the hidden states from the BERT model
    #tranformer is implemented by pytorch and hence different from the other tensorflow frmaworks there is no inference(prediction) part so we have to explicity stop the gredient process with no_grad() 
    with torch.no_grad():
        output = model(**inputs)

    
    #extract the embeddings(vector) corresponding to the [CLS] token for each sentences
    cls_embeddings = output.last_hidden_state[:, 0, :]
    
    return cls_embeddings


#example usage
sentences = [
    "A transformer is a deep learning architecture introduced in the 'Attention is All You Need' paper. It's used for sequence-to-sequence tasks in natural language processing, leveraging self-attention mechanisms to capture contextual relationships",
    # "Here is another sentence"
    # "is this the first sentence"
    "A transformer is a type of deep learning architecture designed for sequence processing tasks. Unlike traditional recurrent neural networks, it utilizes self-attention mechanisms to capture contextual dependencies, enabling more effective modeling of long-range dependencies in sequences."
]

embeddings = get_sentence_embeddings( sentences )


# for i, embedding in enumerate(embeddings):
#     print(f"embedding of sentence {i+1} : {embedding}")

# print(embeddings.shape)


def get_similarity( vect1, vect2 ):
    
    vect1 = np.array(vect1).reshape(1,-1)
    vect2 = np.array(vect2).reshape(1,-1)

    similarity = cosine_similarity( vect1, vect2 )

    return similarity



print( get_similarity(embeddings[0], embeddings[1] ))



     